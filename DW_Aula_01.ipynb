{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03f21ba",
   "metadata": {},
   "source": [
    "\n",
    "# ETL com DummyJSON (online) — Users & Carts\n",
    "\n",
    "Este notebook implementa um **pipeline ETL** consumindo dados **online** do [DummyJSON](https://dummyjson.com/).\n",
    "Ele baixa **/users** e **/carts**, salva **cada registro como um arquivo JSON (nomeado pelo `id`)** na pasta do projeto,\n",
    "transforma e integra os dados, e carrega em formatos analíticos (CSV/Parquet/SQLite).\n",
    "\n",
    "> Endpoints usados:  \n",
    "> • `GET https://dummyjson.com/users` (lista de usuários, campos com `id`)  \n",
    "> • `GET https://dummyjson.com/carts` (carrinhos, com `userId` para junção)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e26cc2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Bibliotecas e versões\n",
    "\n",
    "- **`requests`** — chamadas HTTP aos endpoints do DummyJSON  \n",
    "- **`pandas`** — normalização, limpeza, join e exportação  \n",
    "- **`pyarrow`** — suporte opcional para gravar Parquet via `pandas`  \n",
    "- **`sqlite3`** — banco leve para *staging/analytics*  \n",
    "- **`json` / `pathlib` / `glob`** — E/S de arquivos JSON e manipulação de caminhos  \n",
    "- **`matplotlib`** — visualização rápida (ex.: gasto total por usuário)\n",
    "\n",
    "### Instalação (terminal)\n",
    "```bash\n",
    "pip install requests pandas pyarrow matplotlib\n",
    "```\n",
    "\n",
    "**Para quê?**\n",
    "A biblioteca **`requests`** é usada para **interagir programaticamente com serviços web** através do protocolo HTTP. Ela abstrai os detalhes de baixo nível da comunicação (sockets, codificação, tratamento de cabeçalhos) e oferece uma interface simples para:\n",
    "\n",
    "* **Enviar requisições** em diferentes verbos HTTP (`GET`, `POST`, `PUT`, `DELETE`, etc.).\n",
    "* **Personalizar parâmetros e cabeçalhos** (`params`, `headers`, `cookies`, autenticação).\n",
    "* **Enviar e receber dados estruturados** (JSON, XML, formulários).\n",
    "* **Gerenciar erros e respostas** com métodos como `raise_for_status()`.\n",
    "* **Tratar sessões persistentes** (armazenando cookies e autenticação em múltiplas requisições).\n",
    "* **Controle de timeout e retries**, útil em pipelines de dados.\n",
    "\n",
    "No contexto de ETL, o `requests` é geralmente usado para **extrair dados de APIs** (como DummyJSON) e transformá-los em objetos Python que podem ser processados ou persistidos.\n",
    "\n",
    "**Exemplo prático**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6519d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://dummyjson.com/users/1\"\n",
    "resp = requests.get(url, timeout=30)\n",
    "resp.raise_for_status()           # lança erro se HTTP 4xx/5xx\n",
    "data = resp.json()                # dict Python a partir do JSON\n",
    "print(data[\"firstName\"], data[\"lastName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# endpoint de usuários\n",
    "url = \"https://dummyjson.com/users\"\n",
    "\n",
    "# GET com parâmetros\n",
    "resp = requests.get(url, params={\"limit\": 3}, timeout=10)\n",
    "\n",
    "# verifica se a requisição foi bem-sucedida\n",
    "resp.raise_for_status()\n",
    "\n",
    "# transforma JSON em dict/list Python\n",
    "dados = resp.json()\n",
    "\n",
    "for user in dados[\"users\"]:\n",
    "    print(f\"{user['id']}: {user['firstName']} {user['lastName']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895da9b2",
   "metadata": {},
   "source": [
    "### `pandas` — análise e transformação tabular\n",
    "\n",
    "**Para quê?**\n",
    "A biblioteca **`pandas`** é um dos pilares da ciência de dados em Python. Ela fornece estruturas de dados eficientes (`Series` e `DataFrame`) para **representar e manipular dados tabulares**, funcionando como uma espécie de “Excel programável”. Com ela é possível:\n",
    "\n",
    "* **Ler e escrever** dados em diversos formatos (CSV, JSON, Excel, SQL, Parquet, etc.).\n",
    "* **Normalizar e limpar** dados brutos (remover nulos, renomear colunas, converter tipos).\n",
    "* **Realizar operações estatísticas** e agregações (`mean`, `sum`, `count`, etc.).\n",
    "* **Transformar e combinar** dados com `merge`, `join`, `concat`, `pivot` e `groupby`.\n",
    "* **Selecionar e filtrar** linhas e colunas de forma intuitiva (`loc`, `iloc`, máscaras lógicas).\n",
    "* **Exportar** resultados em formatos otimizados (CSV para portabilidade, Parquet para performance).\n",
    "\n",
    "No contexto de ETL, o `pandas` atua principalmente na fase de **T (Transform)**, preparando os dados para análises ou carregamento em outro sistema.\n",
    "\n",
    "**Exemplo básico**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eff9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "registros = [{\"userId\": 1, \"total\": 100.0}, {\"userId\": 2, \"total\": 50.5}, {\"userId\": 1, \"total\": 25.0}]\n",
    "df = pd.DataFrame(registros)\n",
    "print(df.groupby(\"userId\")[\"total\"].sum())  # soma por usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c555ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# dados brutos (poderiam ter vindo de um JSON)\n",
    "registros = [\n",
    "    {\"userId\": 1, \"total\": 100.0},\n",
    "    {\"userId\": 2, \"total\": 50.5},\n",
    "    {\"userId\": 1, \"total\": 25.0},\n",
    "]\n",
    "\n",
    "# cria DataFrame\n",
    "df = pd.DataFrame(registros)\n",
    "\n",
    "# agrega: soma o total por usuário\n",
    "resumo = df.groupby(\"userId\")[\"total\"].sum().reset_index()\n",
    "\n",
    "print(resumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a5d7f",
   "metadata": {},
   "source": [
    "### `pyarrow` — suporte a Parquet/Arrow (opcional no pandas)\n",
    "\n",
    "**Para quê?**\n",
    "A biblioteca **`pyarrow`** é a implementação Python do **Apache Arrow**, um formato de dados colunar em memória projetado para **alto desempenho e interoperabilidade entre sistemas de análise**. Dentro de um pipeline ETL, ela é usada principalmente para:\n",
    "\n",
    "* **Serialização e desserialização em formatos eficientes** como **Parquet** e **Feather**, reduzindo espaço em disco e acelerando leitura/escrita.\n",
    "* **Conversão entre pandas DataFrames e tabelas Arrow**, preservando tipos e otimizando operações de I/O.\n",
    "* **Interoperabilidade** com outros ecossistemas de Big Data (Spark, Dask, Hadoop, etc.) graças ao padrão Arrow.\n",
    "* **Manipulação de dados em memória** de forma colunar, o que melhora a performance de operações vetorizadas.\n",
    "\n",
    "O `pandas` delega a gravação em Parquet para bibliotecas externas como `pyarrow` ou `fastparquet`. Por isso, instalar `pyarrow` é recomendado sempre que o fluxo de ETL envolve **arquivos grandes ou integração com sistemas analíticos**.\n",
    "\n",
    "**Exemplo básico (via pandas)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37deb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"userId\":[1,2], \"total\":[125.0, 50.5]})\n",
    "df.to_parquet(\"out.parquet\", index=False)   # requer pyarrow instalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"userId\": [1, 2],\n",
    "    \"total\": [125.0, 50.5]\n",
    "})\n",
    "\n",
    "# grava DataFrame em formato Parquet (compacto e otimizado)\n",
    "df.to_parquet(\"out.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "# lê de volta\n",
    "novo_df = pd.read_parquet(\"out.parquet\")\n",
    "print(novo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1de1f1",
   "metadata": {},
   "source": [
    "**Exemplo direto com PyArrow**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dee4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# cria tabela Arrow a partir de um DataFrame pandas\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "# grava em Parquet\n",
    "pq.write_table(table, \"out_direct.parquet\")\n",
    "\n",
    "# lê de volta\n",
    "parquet_file = pq.ParquetFile(\"out_direct.parquet\")\n",
    "print(parquet_file.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc19b1",
   "metadata": {},
   "source": [
    "### `sqlite3` — banco SQL embutido (staging/analytics leve)\n",
    "\n",
    "**Para quê?**\n",
    "O módulo **`sqlite3`** (parte da biblioteca padrão do Python) provê uma interface para o **SQLite**, um banco de dados relacional leve, embutido em um único arquivo `.db`. Ele não exige servidor separado, sendo ideal para pipelines ETL em que se deseja:\n",
    "\n",
    "* **Persistir dados localmente** durante a fase de *staging* (pré-análise) sem depender de sistemas externos.\n",
    "* **Executar consultas SQL completas** (JOINs, agregações, subqueries, etc.) para transformação e análise.\n",
    "* **Integrar com pandas**, exportando/importando dados diretamente de/para DataFrames.\n",
    "* **Trabalhar com portabilidade**: basta distribuir o arquivo `.db` para replicar o dataset.\n",
    "* **Testar protótipos** antes de migrar para um banco mais robusto (PostgreSQL, MySQL, etc.).\n",
    "\n",
    "No fluxo ETL, o `sqlite3` é usado como **área intermediária (staging)** para organizar e validar dados antes de carregá-los em um data warehouse ou ferramenta analítica.\n",
    "\n",
    "[Documentação oficial do módulo sqlite3 (Python)](https://docs.python.org/3/library/sqlite3.html)\n",
    "[Documentação do SQLite](https://www.sqlite.org/docs.html)\n",
    "\n",
    "**Exemplo básico**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38781587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# cria (ou abre) banco local\n",
    "con = sqlite3.connect(\"staging.db\")\n",
    "\n",
    "# cria DataFrame (simulando dados de carrinhos de compra)\n",
    "df = pd.DataFrame({\n",
    "    \"userId\": [1, 2, 1],\n",
    "    \"total\": [100.0, 50.5, 25.0]\n",
    "})\n",
    "\n",
    "# grava DataFrame em uma tabela SQL\n",
    "df.to_sql(\"carts\", con, if_exists=\"replace\", index=False)\n",
    "\n",
    "# consulta SQL: gasto total por usuário\n",
    "resumo = pd.read_sql(\"\"\"\n",
    "    SELECT userId, SUM(total) AS total\n",
    "    FROM carts\n",
    "    GROUP BY userId\n",
    "\"\"\", con)\n",
    "\n",
    "print(resumo)\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55055a79",
   "metadata": {},
   "source": [
    "### `json` / `pathlib` / `glob` — E/S de arquivos e caminhos\n",
    "\n",
    "**Para quê?**\n",
    "\n",
    "* **`json`**:\n",
    "\n",
    "  * Serializa (`dump`, `dumps`) objetos Python (dict/list) em texto JSON.\n",
    "  * Desserializa (`load`, `loads`) strings ou arquivos JSON em estruturas Python.\n",
    "  * Útil em pipelines ETL para gravar dados intermediários ou consumir respostas de APIs.\n",
    "  * [Documentação oficial](https://docs.python.org/3/library/json.html)\n",
    "\n",
    "* **`pathlib`**:\n",
    "\n",
    "  * Fornece uma API **orientada a objetos** para manipulação de caminhos de arquivos e diretórios.\n",
    "  * Facilita operações multiplataforma (Windows, Linux, macOS) sem se preocupar com barras invertidas/duplas.\n",
    "  * Permite criar pastas (`mkdir`), concatenar caminhos (`/`), ler/escrever arquivos diretamente (`write_text`, `read_text`).\n",
    "  * [Documentação oficial](https://docs.python.org/3/library/pathlib.html)\n",
    "\n",
    "* **`glob`**:\n",
    "\n",
    "  * Faz busca de arquivos com **padrões de nomeação** (ex.: `*.json`, `**/*.parquet`).\n",
    "  * Útil para varrer diretórios de *landing* ou *staging* e aplicar ETL em lote.\n",
    "  * Trabalha integrado com `pathlib` para recuperar e manipular coleções de arquivos.\n",
    "  * [Documentação oficial](https://docs.python.org/3/library/glob.html)\n",
    "\n",
    "No contexto ETL, esse trio cobre a **camada de persistência em arquivos**: salvar dados em JSON, navegar na estrutura de diretórios e iterar sobre conjuntos de arquivos de forma eficiente e multiplataforma.\n",
    "\n",
    "**Exemplo básico**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, glob\n",
    "\n",
    "# define pasta de dados\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# salva um dicionário em JSON\n",
    "payload = {\"id\": 1, \"name\": \"Alice\"}\n",
    "(data_dir / \"1.json\").write_text(\n",
    "    json.dumps(payload, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "# percorre todos os arquivos JSON na pasta\n",
    "for path in glob.glob(str(data_dir / \"*.json\")):\n",
    "    obj = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "    print(path, \"->\", obj[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b705d21",
   "metadata": {},
   "source": [
    "Esse conjunto cobre operações típicas de **E/S de arquivos no ETL**:\n",
    "\n",
    "1. **`json`** garante interoperabilidade dos dados.\n",
    "2. **`pathlib`** organiza diretórios de *landing* e *staging*.\n",
    "3. **`glob`** permite iterar sobre arquivos em lote para transformações e carregamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a8df0",
   "metadata": {},
   "source": [
    "### `matplotlib` — visualização rápida\n",
    "\n",
    "**Para quê?**\n",
    "A biblioteca **`matplotlib`** é a base da visualização científica em Python. Ela permite **construir gráficos estáticos, animados e interativos**, com controle detalhado sobre todos os elementos visuais. Mesmo que existam bibliotecas de mais alto nível (`seaborn`, `plotly`), o `matplotlib` continua sendo a “camada fundamental” usada nos bastidores.\n",
    "\n",
    "Em um fluxo ETL, sua principal função é a etapa de **A (Analytics/Visualization)**, possibilitando:\n",
    "\n",
    "* **Criar gráficos básicos**: barras, linhas, dispersão, histogramas.\n",
    "* **Customizar estilos**: cores, eixos, rótulos, títulos, legendas, escalas logarítmicas.\n",
    "* **Integrar com pandas**: DataFrames podem chamar diretamente `.plot()` para gerar gráficos rápidos.\n",
    "* **Salvar visualizações** em formatos como PNG, PDF e SVG para relatórios.\n",
    "* **Exploração ad-hoc** de dados transformados antes de carregá-los em relatórios mais robustos (Power BI, Tableau).\n",
    "\n",
    "[Documentação oficial](https://matplotlib.org/stable/gallery/index.html)\n",
    "\n",
    "**Exemplo básico**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "usuarios = [\"u1\", \"u2\", \"u3\"]\n",
    "totais = [125.0, 50.5, 80.0]\n",
    "\n",
    "plt.bar(usuarios, totais, color=\"skyblue\")\n",
    "plt.title(\"Gasto total por usuário\")\n",
    "plt.xlabel(\"Usuário\")\n",
    "plt.ylabel(\"Total\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af08ff",
   "metadata": {},
   "source": [
    "**Exemplo integrado com pandas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"userId\": [1, 2, 3],\n",
    "    \"total\": [125.0, 50.5, 80.0]\n",
    "})\n",
    "\n",
    "# gera gráfico direto do DataFrame\n",
    "df.plot(kind=\"bar\", x=\"userId\", y=\"total\", legend=False, title=\"Gasto total por usuário\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7ecf0",
   "metadata": {},
   "source": [
    "### Documentação\n",
    "\n",
    "* [Requests: HTTP for Humans - Read the Docs](https://requests.readthedocs.io/)\n",
    "* [pandas 2.3.2 documentation](https://pandas.pydata.org/docs/)\n",
    "* [Reading and Writing the Apache Parquet Format](https://arrow.apache.org/docs/python/parquet.html)\n",
    "* [pyarrow.parquet.ParquetFile — Apache Arrow v21.0.0](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html)\n",
    "* [sqlite3 — DB-API 2.0 interface for SQLite databases](https://docs.python.org/3/library/sqlite3.html)\n",
    "* [SQLite Documentation](https://www.sqlite.org/docs.html)\n",
    "* [JSON encoder and decoder — Python 3.13.7 documentation](https://docs.python.org/3/library/json.html)\n",
    "* [pathlib — Object-oriented filesystem paths](https://docs.python.org/3/library/pathlib.html)\n",
    "* [glob — Unix style pathname pattern expansion](https://docs.python.org/3/library/glob.html)\n",
    "* [Examples — Matplotlib 3.10.6 documentation](https://matplotlib.org/stable/gallery/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9facded",
   "metadata": {},
   "source": [
    "# Cenário Prático de ETL\n",
    "\n",
    "Nesta próxima etapa, vamos aplicar na prática o fluxo **ETL (Extract, Transform, Load)** utilizando uma fonte de dados real. Para isso, será utilizada a [**DummyJSON API**](https://dummyjson.com/), um serviço público que disponibiliza **endpoints de teste com dados simulados** em formato JSON (ex.: usuários, produtos, carrinhos de compras).\n",
    "\n",
    "Essa API é ideal para fins didáticos, pois:\n",
    "\n",
    "* Retorna respostas rápidas em JSON estruturado.\n",
    "* Contém diferentes domínios de dados (usuários, posts, produtos, etc.).\n",
    "* Permite paginação e filtros básicos, simulando o comportamento de APIs reais.\n",
    "\n",
    "Nosso fluxo será o seguinte:\n",
    "\n",
    "1. **Extract (Extração)** — coletar dados de endpoints da DummyJSON com a biblioteca `requests`.\n",
    "2. **Transform (Transformação)** — estruturar, limpar e consolidar os dados em **pandas DataFrames**, explorando operações como normalização e agregação.\n",
    "3. **Load (Carregamento)** — salvar os dados transformados em diferentes formatos:\n",
    "\n",
    "   * **JSON local**, para *landing zone*.\n",
    "   * **SQLite**, como área de *staging*.\n",
    "   * **Parquet**, para análises e interoperabilidade.\n",
    "4. **Analytics/Visualization** — gerar relatórios visuais simples com `matplotlib` para validar os resultados.\n",
    "\n",
    "---\n",
    "\n",
    "### Instalação das bibliotecas\n",
    "\n",
    "Antes de iniciar, verifique se todas as bibliotecas necessárias estão instaladas no ambiente:\n",
    "\n",
    "```bash\n",
    "pip install requests pandas pyarrow matplotlib\n",
    "```\n",
    "\n",
    "> Observação: as bibliotecas `json`, `sqlite3`, `pathlib` e `glob` já fazem parte da **biblioteca padrão do Python**, portanto não precisam ser instaladas separadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49630e66",
   "metadata": {},
   "source": [
    "## 1) Conferindo as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando versões\n",
    "import importlib\n",
    "\n",
    "def safe_version(pkg):\n",
    "    try:\n",
    "        m = importlib.import_module(pkg)\n",
    "        return getattr(m, \"__version__\", \"sem __version__\")\n",
    "    except Exception as e:\n",
    "        return f\"não encontrado ({e.__class__.__name__})\"\n",
    "\n",
    "print({\n",
    "    \"pandas\": safe_version(\"pandas\"),\n",
    "    \"requests\": safe_version(\"requests\"),\n",
    "    \"pyarrow\": safe_version(\"pyarrow\"),\n",
    "    \"matplotlib\": safe_version(\"matplotlib\"),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146c25e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Extração (E) — baixar e salvar **um arquivo por registro**\n",
    "\n",
    "Regras:\n",
    "- Sempre online (DummyJSON)\n",
    "- Os arquivos são salvos **no mesmo local do código** (subpastas `data/raw/users` e `data/raw/carts`)\n",
    "- Nome do arquivo = **`{id}.json`**\n",
    "\n",
    "Abaixo, as funções:\n",
    "- `fetch_all_paged` — varre a API usando `limit/skip` até trazer todos os registros\n",
    "- `save_records_one_per_file` — grava um JSON por registro (nomeado por `id`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c45c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Diretório base onde os arquivos serão salvos\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://dummyjson.com\"\n",
    "\n",
    "def fetch_and_save(endpoints, params=None, out_dir=DATA_DIR):\n",
    "    \"\"\"\n",
    "    Faz download de dados da DummyJSON a partir de múltiplos endpoints\n",
    "    e salva cada resposta em arquivo JSON.\n",
    "\n",
    "    Parâmetros:\n",
    "    - endpoints (list[str]): lista de recursos da API, ex.: [\"users\", \"carts\"]\n",
    "    - params (dict): dicionário com parâmetros comuns de consulta (limit, skip, etc.)\n",
    "    - out_dir (Path): diretório base onde salvar os arquivos .json\n",
    "\n",
    "    Para cada endpoint, será criado um arquivo <endpoint>.json\n",
    "    com a resposta completa da API.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results = {}\n",
    "\n",
    "    for ep in endpoints:\n",
    "        url = f\"{BASE_URL}/{ep}\"\n",
    "        print(f\"Buscando: {url} com params={params}\")\n",
    "        resp = requests.get(url, params=params or {}, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        # salva resposta em arquivo\n",
    "        file_path = out_dir / f\"{ep}.json\"\n",
    "        file_path.write_text(\n",
    "            json.dumps(data, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        results[ep] = file_path\n",
    "    return results\n",
    "\n",
    "\n",
    "# Uso prático:\n",
    "# endpoints escolhidos\n",
    "resources = [\"users\", \"carts\"]\n",
    "\n",
    "# parâmetros usados (opcional)\n",
    "query_params = {\"limit\": 5, \"skip\": 0}\n",
    "\n",
    "arquivos = fetch_and_save(resources, params=query_params)\n",
    "\n",
    "print(\"Arquivos salvos:\")\n",
    "for ep, path in arquivos.items():\n",
    "    print(f\"- {ep}: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5f6b6",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Transformação (T) — normalizar, tipar e integrar\n",
    "\n",
    "- Lê os arquivos individuais (`*.json`) e **concatena** em DataFrames (`users_df`, `carts_df`).\n",
    "- Ajusta tipos (datas/números quando relevante) e faz **join** por `users.id = carts.userId`.\n",
    "- Cria colunas derivadas, como totais e contagens de itens por carrinho/usuário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os arquivos salvos da etapa anterior\n",
    "users_data = json.loads(Path(arquivos[\"users\"]).read_text(encoding=\"utf-8\"))\n",
    "carts_data = json.loads(Path(arquivos[\"carts\"]).read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Normalizar para DataFrames\n",
    "users_df = pd.json_normalize(users_data[\"users\"])\n",
    "carts_df = pd.json_normalize(carts_data[\"carts\"])\n",
    "\n",
    "print(\"Usuários:\")\n",
    "print(users_df.head(3))\n",
    "\n",
    "print(\"\\nCarrinhos:\")\n",
    "print(carts_df[[\"id\", \"userId\", \"products\"]].head(3))\n",
    "\n",
    "# Derivar métricas de cada carrinho\n",
    "def cart_summary(produtos):\n",
    "    total_itens = sum(p.get(\"quantity\", 0) for p in produtos)\n",
    "    total_valor = sum(\n",
    "        p.get(\"total\", p.get(\"price\", 0) * p.get(\"quantity\", 0))\n",
    "        for p in produtos\n",
    "    )\n",
    "    return pd.Series({\"cart_items\": total_itens, \"cart_amount\": total_valor})\n",
    "\n",
    "carts_df[[\"cart_items\", \"cart_amount\"]] = carts_df[\"products\"].apply(cart_summary)\n",
    "\n",
    "print(\"\\nCarrinhos com métricas:\")\n",
    "print(carts_df[[\"id\", \"userId\", \"cart_items\", \"cart_amount\"]].head(3))\n",
    "\n",
    "# Relacionar carts → users\n",
    "merged = carts_df.merge(users_df, left_on=\"userId\", right_on=\"id\", how=\"left\")\n",
    "print(\"\\nJoin carts → users:\")\n",
    "print(merged[[\"id_x\", \"userId\", \"cart_items\", \"cart_amount\", \"firstName\", \"lastName\"]].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a980e",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Carga (L) — CSV, Parquet e SQLite\n",
    "\n",
    "Gravamos a camada **curated** e também um **SQLite** de apoio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ca013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar métricas por usuário a partir do DataFrame 'merged'\n",
    "agg_users = merged.groupby(\"userId\").agg(\n",
    "    total_carts=(\"id_x\", \"count\"),          # quantidade de carrinhos\n",
    "    total_items=(\"cart_items\", \"sum\"),      # itens somados\n",
    "    total_amount=(\"cart_amount\", \"sum\")     # valor total\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nAgregado por usuário:\")\n",
    "print(agg_users.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3de511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Diretórios ---\n",
    "CURATED_DIR = Path(\"data/curated\")\n",
    "CURATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR = Path(\"outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "sqlite_path = OUT_DIR / \"dummyjson_etl.sqlite\"\n",
    "\n",
    "# --- Exportar CSV ---\n",
    "users_df.to_csv(CURATED_DIR / \"users.csv\", index=False, encoding=\"utf-8\")\n",
    "carts_df.to_csv(CURATED_DIR / \"carts.csv\", index=False, encoding=\"utf-8\")\n",
    "merged.to_csv(CURATED_DIR / \"carts_users_merged.csv\", index=False, encoding=\"utf-8\")\n",
    "agg_users.to_csv(CURATED_DIR / \"users_agg.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# --- Exportar Parquet (opcional) ---\n",
    "try:\n",
    "    users_df.to_parquet(CURATED_DIR / \"users.parquet\", index=False)\n",
    "    carts_df.to_parquet(CURATED_DIR / \"carts.parquet\", index=False)\n",
    "    merged.to_parquet(CURATED_DIR / \"carts_users_merged.parquet\", index=False)\n",
    "    agg_users.to_parquet(CURATED_DIR / \"users_agg.parquet\", index=False)\n",
    "    print(\"Arquivos Parquet salvos em:\", CURATED_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Parquet indisponível:\", e.__class__.__name__)\n",
    "\n",
    "# --- Função auxiliar: prepara DataFrame para SQLite ---\n",
    "def make_sql_ready(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converte listas/dicionários em JSON string antes de salvar no SQLite.\n",
    "    \"\"\"\n",
    "    df_sql = df.copy()\n",
    "    for col in df_sql.columns:\n",
    "        if df_sql[col].apply(lambda x: isinstance(x, (list, dict))).any():\n",
    "            df_sql[col] = df_sql[col].apply(json.dumps)\n",
    "    return df_sql\n",
    "\n",
    "# --- Exportar para SQLite ---\n",
    "con = sqlite3.connect(sqlite_path)\n",
    "\n",
    "make_sql_ready(users_df).to_sql(\"dim_users\", con, if_exists=\"replace\", index=False)\n",
    "make_sql_ready(carts_df).to_sql(\"fact_carts_raw\", con, if_exists=\"replace\", index=False)\n",
    "make_sql_ready(merged).to_sql(\"fact_carts_enriched\", con, if_exists=\"replace\", index=False)\n",
    "make_sql_ready(agg_users).to_sql(\"agg_users\", con, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Consulta de exemplo\n",
    "q = \"\"\"\n",
    "SELECT userId,\n",
    "       total_carts,\n",
    "       total_items,\n",
    "       total_amount\n",
    "FROM agg_users\n",
    "ORDER BY total_amount DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "preview = pd.read_sql_query(q, con)\n",
    "con.close()\n",
    "\n",
    "print(\"\\nTop 10 usuários por gasto total:\")\n",
    "print(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205bd7d6",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Visualização rápida\n",
    "\n",
    "Um gráfico simples com **total gasto por usuário**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd95ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ordena os usuários pelo gasto total (decrescente)\n",
    "top_users = agg_users.sort_values(\"total_amount\", ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(top_users[\"userId\"].astype(str), top_users[\"total_amount\"], color=\"steelblue\")\n",
    "\n",
    "plt.title(\"Top 10 usuários por gasto total\")\n",
    "plt.xlabel(\"Usuário\")\n",
    "plt.ylabel(\"Gasto total\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddc7af",
   "metadata": {},
   "source": [
    "## Recap do Pipeline ETL\n",
    "\n",
    "* **Extração (E)**\n",
    "\n",
    "  * Fonte: [DummyJSON](https://dummyjson.com/) (API pública online).\n",
    "  * Estratégia: paginação com `limit`/`skip`, *retry* em caso de falha.\n",
    "  * Persistência inicial (*landing*): **um arquivo JSON por registro** (`{id}.json`).\n",
    "\n",
    "* **Transformação (T)**\n",
    "\n",
    "  * Leitura e **normalização** dos arquivos JSON em `pandas.DataFrame`.\n",
    "  * Derivação de métricas por carrinho (`cart_items`, `cart_amount`).\n",
    "  * Integração entre entidades com **join** (`userId` ↔ `id`).\n",
    "\n",
    "* **Carga (L)**\n",
    "\n",
    "  * Exportação para **CSV** (portabilidade e inspeção rápida).\n",
    "  * Exportação para **Parquet** (colunar, otimizado para análise, quando `pyarrow` disponível).\n",
    "  * Persistência em **SQLite** (área de staging), com tabelas:\n",
    "\n",
    "    * `dim_users` (dimensão usuários)\n",
    "    * `fact_carts_raw` (fato carrinhos normalizado)\n",
    "    * `fact_carts_enriched` (carrinhos + dados de usuários)\n",
    "    * `agg_users` (agregação por usuário)\n",
    "\n",
    "* **Análise (A)**\n",
    "\n",
    "  * Consultas SQL para validar o staging.\n",
    "  * **Gráfico de barras** (Matplotlib) mostrando o total gasto por usuário.\n",
    "  * Permite inspecionar rapidamente a **qualidade do pipeline** e gerar insights iniciais.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf9d53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09114220",
   "metadata": {},
   "source": [
    "# Desafio: Criando uma Dimensão de Produtos e Analisando Consumo\n",
    "\n",
    "A empresa fictícia **ShopX** precisa entender melhor o comportamento de compra dos seus clientes. Para isso, você deve expandir o Data Warehouse existente com base na **DummyJSON API**.\n",
    "\n",
    "### O que precisa ser feito\n",
    "\n",
    "1. **Extração (E):**\n",
    "\n",
    "   * Além de usuários e carrinhos, agora também será necessário coletar **produtos** da API.\n",
    "   * Cada produto tem atributos como título, preço, categoria e marca.\n",
    "\n",
    "2. **Transformação (T):**\n",
    "\n",
    "   * Criar uma **dimensão de produtos (`dim_products`)** a partir dos dados coletados.\n",
    "   * Garantir que os atributos sejam tipados corretamente (números, strings, categorias).\n",
    "   * Relacionar os produtos com os carrinhos, criando uma **tabela fato detalhada** onde cada linha representa um produto dentro de um carrinho.\n",
    "\n",
    "3. **Carga (L):**\n",
    "\n",
    "   * Integrar as novas tabelas ao Data Warehouse local (CSV, Parquet e SQLite).\n",
    "   * Atualizar a modelagem dimensional com a nova estrela:\n",
    "\n",
    "     * **Dimensões:** usuários, produtos.\n",
    "     * **Fatos:** itens de carrinho.\n",
    "\n",
    "4. **Análise (A):**\n",
    "\n",
    "   * Descobrir quais são as **categorias de produtos mais vendidas** em termos de quantidade e faturamento.\n",
    "   * Identificar o **ticket médio por categoria**.\n",
    "   * Comparar os padrões de compra entre diferentes perfis de usuários (ex.: gênero ou faixa etária).\n",
    "\n",
    "---\n",
    "\n",
    "### Regras do desafio\n",
    "\n",
    "* O pipeline deve ser **reprodutível** e não depender de dados externos além da DummyJSON.\n",
    "* O Data Warehouse precisa ter ao menos **uma dimensão nova (produtos)** e **uma tabela fato granular (itens de carrinho)**.\n",
    "* A análise final deve gerar **pelo menos 3 insights quantitativos** a partir do DW com visualização.\n",
    "\n",
    "## Construa seu modelo abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6c53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12ff7dd9",
   "metadata": {},
   "source": [
    "## Resultado da Análise — Top Categorias por Faturamento (Exemplo de resultado)\n",
    "\n",
    "**Dados coletados:**\n",
    "\n",
    "* 50 usuários\n",
    "* 50 carrinhos\n",
    "* 50 produtos\n",
    "\n",
    "**Consulta OLAP:** gasto total agregado por categoria de produto.\n",
    "\n",
    "| Categoria           | Total de Itens | Faturamento Total |\n",
    "| ------------------- | -------------- | ----------------- |\n",
    "| furniture           | 9              | 12,699.91         |\n",
    "| fragrances          | 14             | 1,059.86          |\n",
    "| home-decoration     | 22             | 639.78            |\n",
    "| groceries           | 99             | 410.31            |\n",
    "| beauty              | 17             | 193.83            |\n",
    "| kitchen-accessories | 8              | 59.92             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90509b0e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
